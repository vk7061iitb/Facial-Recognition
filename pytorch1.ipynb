{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61acd0f0-1a4e-4069-9320-726872408c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe9d550-27ad-478f-afab-81eb0d6b0c21",
   "metadata": {},
   "source": [
    "`pytorch` is a python package which enables users to train state-of-the-art machine learning/deep learning models. In order to efficiently use `pytorch`, one needs to have a firm understanding of the basic building blocks of pytorch: the `torch.tensor` object. In many ways, it's similar to a numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6820ab40-dfb6-4c7b-8c76-ca830e00da77",
   "metadata": {},
   "source": [
    "# Numpy vs. Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f402f-edb8-4b20-95e7-95bae3714f34",
   "metadata": {},
   "source": [
    "Numpy **`array`s** and pytorch **`tensor`s** can be created in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f46cd458-bbb9-44a8-a672-9c12bb308283",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.linspace(0,1,5)\n",
    "t = torch.linspace(0,1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67d4de13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.25, 0.5 , 0.75, 1.  ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ba4c71-663d-4791-be93-2a2fb1bccf0d",
   "metadata": {},
   "source": [
    "They can be resized in similar ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fd84a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.2500, 0.5000, 0.7500, 1.0000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91f66aa7-a237-4151-b98e-bb08d9a27ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.arange(48).reshape(3,4,4)\n",
    "t = torch.arange(48).reshape(3,4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96e3a4d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11],\n",
       "        [12, 13, 14, 15]],\n",
       "\n",
       "       [[16, 17, 18, 19],\n",
       "        [20, 21, 22, 23],\n",
       "        [24, 25, 26, 27],\n",
       "        [28, 29, 30, 31]],\n",
       "\n",
       "       [[32, 33, 34, 35],\n",
       "        [36, 37, 38, 39],\n",
       "        [40, 41, 42, 43],\n",
       "        [44, 45, 46, 47]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b328e1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3],\n",
       "         [ 4,  5,  6,  7],\n",
       "         [ 8,  9, 10, 11],\n",
       "         [12, 13, 14, 15]],\n",
       "\n",
       "        [[16, 17, 18, 19],\n",
       "         [20, 21, 22, 23],\n",
       "         [24, 25, 26, 27],\n",
       "         [28, 29, 30, 31]],\n",
       "\n",
       "        [[32, 33, 34, 35],\n",
       "         [36, 37, 38, 39],\n",
       "         [40, 41, 42, 43],\n",
       "         [44, 45, 46, 47]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3739c336-9d1e-47f7-b0b8-99e37cfefdf3",
   "metadata": {},
   "source": [
    "Most importantly, they have the same broadcasting rules. In order to use `pytorch` (and even `numpy` for that matter) most efficiently, one needs to have a very strong grasp on the **broadcasting rules**.\n",
    "\n",
    "# General Broadcasting Rules\n",
    "\n",
    "When operating on two arrays, NumPy compares their shapes element-wise. It starts with the trailing (i.e. rightmost) dimensions and works its way left. Two dimensions are compatible when\n",
    "\n",
    "1. they are equal, or\n",
    "2. one of them is 1\n",
    "\n",
    "**Example**: The following are compatible\n",
    "\n",
    "Shape 1: (1,6,4,1,7,2)\n",
    "\n",
    "Shape 2: (5,6,1,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcfe77a3-20b8-4e90-90bd-283fb433ba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones((6,5))\n",
    "b = np.arange(5).reshape((1,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10c5d6ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf7d0024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9b07e45-40bd-4ca0-a897-ed2d8ed92533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.],\n",
       "       [1., 2., 3., 4., 5.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beebe945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2., 3., 4.],\n",
       "       [0., 1., 2., 3., 4.],\n",
       "       [0., 1., 2., 3., 4.],\n",
       "       [0., 1., 2., 3., 4.],\n",
       "       [0., 1., 2., 3., 4.],\n",
       "       [0., 1., 2., 3., 4.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dca407e-2aa3-49a5-be7a-cc311702d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((6,5))\n",
    "b = torch.arange(5).reshape((1,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c3987a-2600-4de1-8df2-c0428026c26d",
   "metadata": {},
   "source": [
    "The arrays/tensors don't need to have the same number of dimenions. If one of the arrays/tensors has less dimensions than the other\n",
    "\n",
    "**Example**: Scaling each other the color channels of an image by a different amount:\n",
    "\n",
    "<pre><span></span><span class=\"n\">Image</span>  <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"n\">d</span> <span class=\"n\">array</span><span class=\"p\">):</span> <span class=\"mi\">256</span> <span class=\"n\">x</span> <span class=\"mi\">256</span> <span class=\"n\">x</span> <span class=\"mi\">3</span>\n",
    "<span class=\"n\">Scale</span>  <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"n\">d</span> <span class=\"n\">array</span><span class=\"p\">):</span>             <span class=\"mi\">3</span>\n",
    "<span class=\"n\">Result</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"n\">d</span> <span class=\"n\">array</span><span class=\"p\">):</span> <span class=\"mi\">256</span> <span class=\"n\">x</span> <span class=\"mi\">256</span> <span class=\"n\">x</span> <span class=\"mi\">3</span>\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae4d7516-f207-4c19-bea3-5463b0bcc60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image = torch.randn((256,256,3))\n",
    "Scale = torch.tensor([0.5,1.5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbc95f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.3660e-01, -9.5518e-01,  2.9510e-01],\n",
       "         [ 1.9270e+00,  1.0877e+00,  4.2436e-01],\n",
       "         [ 8.0238e-04,  6.5404e-01, -8.9061e-01],\n",
       "         ...,\n",
       "         [-1.3292e+00, -2.2048e-01,  1.0885e+00],\n",
       "         [-1.1427e+00,  1.4648e-01,  5.8870e-01],\n",
       "         [ 1.9833e+00,  1.5891e+00,  1.9013e+00]],\n",
       "\n",
       "        [[ 1.5299e+00, -5.6662e-01, -7.3423e-01],\n",
       "         [ 1.2475e-01,  2.1822e-01,  2.3908e-01],\n",
       "         [ 1.2213e+00, -8.5528e-01,  1.0595e+00],\n",
       "         ...,\n",
       "         [ 1.1050e+00,  1.6084e+00,  5.6432e-01],\n",
       "         [-2.4785e-01, -5.7377e-01,  7.7358e-01],\n",
       "         [ 3.4233e-02, -1.0948e+00, -2.6357e+00]],\n",
       "\n",
       "        [[-1.1892e+00,  7.1152e-01, -1.7434e-01],\n",
       "         [-6.1290e-01,  1.2436e+00,  1.1823e+00],\n",
       "         [ 1.0826e-01, -1.4757e+00, -7.9655e-01],\n",
       "         ...,\n",
       "         [-8.5882e-01, -1.8992e-01, -2.2546e+00],\n",
       "         [ 1.2440e+00, -1.4400e-01, -8.1545e-01],\n",
       "         [-1.2190e+00,  8.0561e-01,  1.0008e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 4.7313e-01, -6.6341e-01, -2.6003e-01],\n",
       "         [ 1.3011e+00,  1.1876e+00, -1.8890e+00],\n",
       "         [-4.1185e-01, -4.4728e-01,  3.9634e-01],\n",
       "         ...,\n",
       "         [ 8.4723e-01, -1.1262e+00, -5.5287e-02],\n",
       "         [ 8.0220e-01, -2.2736e-01,  1.1402e+00],\n",
       "         [ 7.5771e-01,  1.8385e-01,  5.3593e-01]],\n",
       "\n",
       "        [[-7.1972e-01, -1.7879e+00, -1.4604e+00],\n",
       "         [ 1.2781e+00,  6.3198e-02,  2.0303e+00],\n",
       "         [-1.1235e+00, -1.5182e+00, -1.5369e+00],\n",
       "         ...,\n",
       "         [ 1.5461e+00, -7.4892e-01,  1.2983e+00],\n",
       "         [ 7.9914e-01,  6.6116e-01, -1.3694e-01],\n",
       "         [ 1.4445e+00, -7.2986e-02,  5.2687e-01]],\n",
       "\n",
       "        [[ 1.7370e+00, -7.6628e-01, -4.4708e-01],\n",
       "         [ 2.6179e-01,  2.6402e-01,  2.0551e+00],\n",
       "         [-2.5554e+00, -6.1255e-01, -9.2255e-01],\n",
       "         ...,\n",
       "         [-4.7411e-01,  1.2856e+00, -2.2266e-01],\n",
       "         [ 2.2428e+00,  1.7736e+00,  4.7150e-01],\n",
       "         [ 9.6716e-02, -1.3822e+00,  1.5660e-01]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0be27a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 1.5000, 1.0000])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cf7a995-6ab8-475e-a42d-737f1ce3deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Result = Image*Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc69f2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1830e-01, -1.4328e+00,  2.9510e-01],\n",
       "         [ 9.6352e-01,  1.6315e+00,  4.2436e-01],\n",
       "         [ 4.0119e-04,  9.8107e-01, -8.9061e-01],\n",
       "         ...,\n",
       "         [-6.6461e-01, -3.3072e-01,  1.0885e+00],\n",
       "         [-5.7133e-01,  2.1972e-01,  5.8870e-01],\n",
       "         [ 9.9165e-01,  2.3837e+00,  1.9013e+00]],\n",
       "\n",
       "        [[ 7.6495e-01, -8.4993e-01, -7.3423e-01],\n",
       "         [ 6.2376e-02,  3.2733e-01,  2.3908e-01],\n",
       "         [ 6.1064e-01, -1.2829e+00,  1.0595e+00],\n",
       "         ...,\n",
       "         [ 5.5252e-01,  2.4126e+00,  5.6432e-01],\n",
       "         [-1.2393e-01, -8.6066e-01,  7.7358e-01],\n",
       "         [ 1.7116e-02, -1.6423e+00, -2.6357e+00]],\n",
       "\n",
       "        [[-5.9462e-01,  1.0673e+00, -1.7434e-01],\n",
       "         [-3.0645e-01,  1.8654e+00,  1.1823e+00],\n",
       "         [ 5.4130e-02, -2.2136e+00, -7.9655e-01],\n",
       "         ...,\n",
       "         [-4.2941e-01, -2.8488e-01, -2.2546e+00],\n",
       "         [ 6.2198e-01, -2.1600e-01, -8.1545e-01],\n",
       "         [-6.0951e-01,  1.2084e+00,  1.0008e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.3656e-01, -9.9511e-01, -2.6003e-01],\n",
       "         [ 6.5057e-01,  1.7814e+00, -1.8890e+00],\n",
       "         [-2.0592e-01, -6.7092e-01,  3.9634e-01],\n",
       "         ...,\n",
       "         [ 4.2362e-01, -1.6893e+00, -5.5287e-02],\n",
       "         [ 4.0110e-01, -3.4104e-01,  1.1402e+00],\n",
       "         [ 3.7886e-01,  2.7577e-01,  5.3593e-01]],\n",
       "\n",
       "        [[-3.5986e-01, -2.6818e+00, -1.4604e+00],\n",
       "         [ 6.3907e-01,  9.4797e-02,  2.0303e+00],\n",
       "         [-5.6176e-01, -2.2773e+00, -1.5369e+00],\n",
       "         ...,\n",
       "         [ 7.7306e-01, -1.1234e+00,  1.2983e+00],\n",
       "         [ 3.9957e-01,  9.9174e-01, -1.3694e-01],\n",
       "         [ 7.2225e-01, -1.0948e-01,  5.2687e-01]],\n",
       "\n",
       "        [[ 8.6848e-01, -1.1494e+00, -4.4708e-01],\n",
       "         [ 1.3089e-01,  3.9603e-01,  2.0551e+00],\n",
       "         [-1.2777e+00, -9.1882e-01, -9.2255e-01],\n",
       "         ...,\n",
       "         [-2.3705e-01,  1.9284e+00, -2.2266e-01],\n",
       "         [ 1.1214e+00,  2.6605e+00,  4.7150e-01],\n",
       "         [ 4.8358e-02, -2.0733e+00,  1.5660e-01]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b07adba-f7af-4ea5-a88e-93b790b1a2bc",
   "metadata": {},
   "source": [
    "**Example**: One has an array of 2 images and wants to scale the color channels of each image by a slightly different amount:\n",
    "\n",
    "<pre><span></span><span class=\"n\">Images</span>  <span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"n\">d</span> <span class=\"n\">array</span><span class=\"p\">):</span> <span class=\"mi\">2</span> <span class=\"n\">x</span> <span class=\"mi\">256</span> <span class=\"n\">x</span> <span class=\"mi\">256</span> <span class=\"n\">x</span> <span class=\"mi\">3</span>\n",
    "<span class=\"n\">Scales</span>  <span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"n\">d</span> <span class=\"n\">array</span><span class=\"p\">):</span> <span class=\"mi\">2</span> <span class=\"n\">x</span> <span class=\"mi\">1</span> <span class=\"n\">x</span> <span class=\"mi\">1</span> <span class=\"n\">x</span> <span class=\"mi\">3</span>\n",
    "<span class=\"n\">Results</span>  <span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"n\">d</span> <span class=\"n\">array</span><span class=\"p\">):</span> <span class=\"mi\">2</span> <span class=\"n\">x</span> <span class=\"mi\">256</span> <span class=\"n\">x</span> <span class=\"mi\">256</span> <span class=\"n\">x</span> <span class=\"mi\">3</span>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "850677c7-4238-4cdd-a05c-815781c56e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Images = torch.randn((2,256,256,3))\n",
    "Scales = torch.tensor([0.5,1.5,1,1.5,1,0.5]).reshape((2,1,1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2d91c5-75a2-46cc-9003-9a6004daacc2",
   "metadata": {},
   "source": [
    "# Operations Across Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4396c9b2-d118-412d-911d-89352db54fcb",
   "metadata": {},
   "source": [
    "This is so fundamental for pytorch. Obviously simple operations can be done one 1 dimensional tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78f2b3a7-717f-41a8-aa27-8e46ca63cc9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.1250), tensor(1.6520), tensor(4.), tensor(0.5000))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([0.5,1,3,4])\n",
    "torch.mean(t), torch.std(t), torch.max(t), torch.min(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180e72c4-8d53-4c6c-9d94-21eacd5e5a6b",
   "metadata": {},
   "source": [
    "But suppose we have a 2d tensor, for example, and want to compute the mean value of each columns:\n",
    "* Note: taking the mean **of** each column means taking the mean **across** the rows (which are the first dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a494d41b-8239-461a-ab31-9cdfcf9f7c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(20, dtype=float).reshape(5,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8676b94b-91e4-4b2d-94fb-ea6ef5ebc914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.,  9., 10., 11.], dtype=torch.float64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.arange(20, dtype=float).reshape(5,4)\n",
    "torch.mean(t, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3101e33a-0f61-4339-a034-158244473ec0",
   "metadata": {},
   "source": [
    "This can be done for higher dimensionality arrays as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db234a3b-6538-47b6-94fe-b12d04b714e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(5,256,256,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44ac080-bbce-4a71-bbd5-1d0dce136b68",
   "metadata": {},
   "source": [
    "Take the mean across the batch (size 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "810a750f-00db-453e-92c9-83449c6c762c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 256, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t,axis=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e6d067-9469-4875-befa-ada79ca8f6b7",
   "metadata": {},
   "source": [
    "Take the mean across the color channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91ab24be-a8d0-4fc0-a9a7-6d53ab9fb6a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 256, 256])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(t,axis=-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e911073f-6a04-4e01-a9a4-16ebfe9c0073",
   "metadata": {},
   "source": [
    "Take only the maximum color channel values (and get the corresponding indices):\n",
    "* This is done all the time in image segmentation models (i.e. take an image, decide which pixels correspond to, say, a car)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32a40b2c-091e-4196-8e05-d849d04d9867",
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = torch.max(t,axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501750f2-75ae-4c64-b472-4deb38fdfaff",
   "metadata": {},
   "source": [
    "# So Where Do Pytorch and Numpy Differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e2d157-d8ff-4c6a-8ed5-64edb0b57471",
   "metadata": {},
   "source": [
    "**Pytorch** starts to really differ from **numpy** in terms of automatically computing gradients of operations\n",
    "\n",
    "$$y = \\sum_i x_i^3$$\n",
    "\n",
    "has a gradient\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x_i} = 3x_i^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db724997-80bc-4309-8bb1-9f1b84c42f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[5.,8.],[4.,6.]], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49edfec1-092f-4c04-a1c1-298bf1e861e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(917., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[5.,8.],[4.,6.]], requires_grad=True)\n",
    "y = x.pow(3).sum()\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e2768d-8820-4784-abf1-cea8cf4d6ec4",
   "metadata": {},
   "source": [
    "Compute the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "85a51b1c-fb36-48c3-8364-5c6e292de13e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 75., 192.],\n",
       "        [ 48., 108.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward() #compute the gradient\n",
    "x.grad #print the gradient (everything that has happened to x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0491784-4586-4c40-90f2-cce718765fca",
   "metadata": {},
   "source": [
    "Double check using the analytical formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d84d76b4-c504-4dbe-94b3-3b846891c357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 75., 192.],\n",
       "        [ 48., 108.]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*x**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c57eaa-419f-4b92-974b-2a96918eab1f",
   "metadata": {},
   "source": [
    "The automatic computation of gradients is the backbone of training deep learning models. Unlike in the example above, most gradient computations don't have an analytical formula, so the automatic computation of gradients is essential. In general, if one has \n",
    "\n",
    "$$y = f(\\vec{x})$$\n",
    "\n",
    "Then pytorch can compute $\\partial y / \\partial x_i$. For each of element of the vector $\\vec{x}$. In the context of machine learning, $\\vec{x}$ contains all the weights (also known as parameters) of the neural network and $y$ is the **Loss Function** of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a3940a-2267-4d5c-b2ea-3daa1732ecaf",
   "metadata": {},
   "source": [
    "# Additional Benefits\n",
    "\n",
    "**In addition, any sort of large matrix multiplication problem is faster with torch tensors than it is with numpy arrays, especially if you're running on a GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c867df20-dceb-4903-bba9-d2641e01f501",
   "metadata": {},
   "source": [
    "Using torch: (with a CPU. With GPU, this is much much faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d135ad0-ff23-49e6-8d93-1893d12f9e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn((1000,1000))\n",
    "B = torch.randn((1000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c534cfd-6157-4003-a7e2-2837c06e5f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03473169999779202\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "torch.matmul(A,B)\n",
    "t2 = time.perf_counter()\n",
    "print(t2-t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c27819d-9310-46e1-b3d9-779903f19760",
   "metadata": {},
   "source": [
    "Using numpy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ad67a2d-e36b-4ffe-ab66-bc7af7ed7e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randn(int(1e6)).reshape((1000,1000))\n",
    "B = np.random.randn(int(1e6)).reshape((1000,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19db35ae-5bc9-4497-bb7e-fee1c36f8826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0417231999890646\n"
     ]
    }
   ],
   "source": [
    "t1 = time.perf_counter()\n",
    "A@B\n",
    "t2 = time.perf_counter()\n",
    "print(t2-t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
